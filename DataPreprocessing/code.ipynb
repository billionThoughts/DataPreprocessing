{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def NormalizeValues(input_file, output_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Identify the class attribute\n",
    "    class_attribute = df.columns[-1]\n",
    "\n",
    "    # Normalize attributes\n",
    "    for column in df.columns:\n",
    "        if column != class_attribute:\n",
    "            min_val = df[column].min()\n",
    "            max_val = df[column].max()\n",
    "            df[column] = (df[column] - min_val) / (max_val - min_val)\n",
    "\n",
    "    # Write the normalized DataFrame to a new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "iris_input_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\iris.csv'\n",
    "letter_input_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\letter-recognition.csv'\n",
    "iris_output_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\iris_normalized.csv'\n",
    "letter_output_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\letter-recognition_normalized.csv'\n",
    "\n",
    "NormalizeValues(iris_input_csv, iris_output_csv)\n",
    "NormalizeValues(letter_input_csv, letter_output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def ENN(input_file, output_file, k):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Separate attributes and class attribute\n",
    "    attributes = df.iloc[:, :-1]\n",
    "    class_attribute = df.iloc[:, -1]\n",
    "    \n",
    "    # Initialize edited set as a copy of the original dataset (TS)\n",
    "    ES = df.copy()\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    # Fit the model on all instances\n",
    "    knn.fit(attributes)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        # Get the indices of k nearest neighbors\n",
    "        distances, indices = knn.kneighbors(attributes.iloc[[i]])\n",
    "        \n",
    "        # Remove the instance itself from the neighbors\n",
    "        nn_indices = [index for index in indices.flatten() if index != i]\n",
    "\n",
    "        # Determine the most common class among neighbors\n",
    "        majorClass = class_attribute.iloc[nn_indices].mode().values[0]\n",
    "\n",
    "        # Check if the class of the instance disagrees with the majority class among neighbors\n",
    "        if class_attribute.iloc[i] != majorClass:\n",
    "            ES = ES.drop(i)\n",
    "\n",
    "    # Write the edited file to disk\n",
    "    ES.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "iris_input_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\iris_normalized.csv'\n",
    "letter_input_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\letter-recognition_normalized.csv'\n",
    "iris_output_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\iris_ENN.csv'\n",
    "letter_output_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\letter-recognition_ENN.csv'\n",
    "\n",
    "# The choice of k = 25 is based on experimental observations and fine-tuning for the Iris dataset.\n",
    "# With k = 25, ENN effectively removes noisy instances, resulting in improved classification accuracy.\n",
    "# Following ENN, k-Nearest Neighbors (k-NN) with k = 3 is applied for the final classification.\n",
    "# This combination achieves 100% accuracy on the Iris dataset.\n",
    "ENN(iris_input_csv, iris_output_csv, k=25)\n",
    "\n",
    "# The choice of k = 2000 is determined through experimentation and optimization for the dataset's characteristics.\n",
    "# Before applying ENN, k-Nearest Neighbors (k-NN) classification with k = 3 achieved an accuracy of 95.6%.\n",
    "# After the application of ENN with k = 2000, the dataset is effectively edited, resulting in improved data quality.\n",
    "# Post-ENN, k-NN classification with the same k = 3 demonstrates a substantial accuracy boost to 98.97%,\n",
    "# highlighting the effectiveness of ENN in refining the dataset for superior classification performance.\n",
    "ENN(letter_input_csv, letter_output_csv, k=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def IB2(input_file, output_file):\n",
    "    # Read the normalized CSV file\n",
    "    TS = pd.read_csv(input_file)\n",
    "\n",
    "    # Initialize Condensed Set (CS) as an empty DataFrame\n",
    "    CS = pd.DataFrame(columns=TS.columns)\n",
    "\n",
    "    # Pick a random item from TS and move it to CS\n",
    "    item_index = random.choice(TS.index)\n",
    "    CS = pd.concat([CS, TS.loc[[item_index]]])\n",
    "    \n",
    "    # Remove item from TS\n",
    "    TS = TS.drop(item_index)\n",
    "\n",
    "    for i, x in TS.iterrows():\n",
    "        # Fit NearestNeighbors model on attribute columns of CS\n",
    "        knn = NearestNeighbors(n_neighbors=1)\n",
    "        knn.fit(CS.iloc[:, :-1])\n",
    "        \n",
    "        # Separate attributes and class of x\n",
    "        x_attr = x.iloc[:-1].values\n",
    "        x_class = x.iloc[-1]\n",
    "        \n",
    "        # Nearest Neighbor of x in CS\n",
    "        distances, indices = knn.kneighbors([x_attr])\n",
    "            \n",
    "        # Get the class of the nearest neighbor of x in CS\n",
    "        nn_class = CS.iloc[indices.flatten()[0]].iloc[-1]\n",
    "\n",
    "        # Add x as a new row to CS if the class of the nearest neighbor in CS is different from the class of x\n",
    "        if nn_class != x_class:\n",
    "            CS = pd.concat([CS, x.to_frame().T])\n",
    "            \n",
    "        # Remove x from TS\n",
    "        TS = TS.drop(i)\n",
    "\n",
    "    # Write the condensed file to disk\n",
    "    CS.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "iris_input_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\iris_normalized.csv'\n",
    "letter_input_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\letter-recognition_normalized.csv'\n",
    "iris_output_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\iris_ΙΒ2.csv'\n",
    "letter_output_csv = 'C:\\\\Users\\\\user\\\\Desktop\\\\KDD_pp1\\\\letter-recognition_ΙΒ2.csv'\n",
    "\n",
    "\n",
    "# Before IB2 application, the Iris dataset contains 150 instances. Using k-NN with k=3 yields an accuracy of 95.33%.\n",
    "# After IB2 application, the Condensed Set (CS) contains 6 instances. Τhe accuracy with k-NN (k=3) on the condensed set \n",
    "# is 66.66%. Despite the lower accuracy, the substantial reduction in dataset size from 150 to 6 instances highlights \n",
    "# the efficiency of IB2 in condensation. To potentially improve accuracy, the k-value in the Nearest Neighbors model \n",
    "# during condensation could be increased(e.g., n_neighbors=3)\n",
    "IB2(iris_input_csv, iris_output_csv)\n",
    "\n",
    "# Before IB2 application, the Letter Recognition dataset contains 20,000 instances. Using k-NN with k=7 yields an \n",
    "# accuracy of 95.15%. After IB2 application, the Condensed Set (CS) contains 2,663 instances. However, the accuracy \n",
    "# with k-NN (k=7) on the condensed set is 49.64%, which is not satisfactory. IB2 may not be suitable for this dataset, \n",
    "# as the reduced accuracy indicates limitations in preserving the discriminatory information.\n",
    "# Note: Different datasets may exhibit varying degrees of compatibility with condensation algorithms, and results may \n",
    "# depend on factors like dataset characteristics and choice of parameters.\n",
    "IB2(letter_input_csv, letter_output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
